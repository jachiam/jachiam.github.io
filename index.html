<html>
<head>
<style>
table, th, td {
    border: 1px solid black;
}
td {
    padding: 10px;
}
</style>
</head>
<body>

<table style="width:80%">

<tr>

<td colspan=2>
<br>
Hi, I'm Joshua Achiam. I'm currently a researcher on the Safety Team at <a href="https://openai.com/">OpenAI</a>, and a sixth-year PhD student at UC Berkeley in <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel's group</a>.
</ul>

<br>
<br>
</td>

</tr>
<tr>
<td>
<b>Interests</b>
</td>
<td>
<br>
I am primarily interested in a path to safe AGI. If AGI is realizable in the near future (anytime within the next century), it's really important that we make sure it comes into existence in a way which doesn't cause harm to humanity. Consequently I pick research topics that I think will increase our understanding of AGI and help us decrease the risk of bad outcomes. Currently I am studying topics in deep reinforcement learning (deep RL), because I personally find it likely that deep RL will be a component of future AGIs. 
<br>
<br>
</td>
</tr>
<tr>
<td>
<b>Research</b>
</td>

<td>

<br>
<ul>

<li> <b>Variational Autoencoding Learning of Options by Reinforcement</b>, <br>
with Harrison Edwards, Dario Amodei, and Pieter Abbeel. <br>
NIPS 2017 Deep RL Symposium. (<a href="https://drive.google.com/file/d/1mq6vpSNylfbFG4A1MOdv9czGMVuB7jc6/view">Google Drive Link</a>)<br><br>
</li>

<li> <b>Constrained Policy Optimization</b>,<br>
 with Dave Held, Aviv Tamar, and Pieter Abbeel. <br>
ICML 2017. (<a href="https://arxiv.org/abs/1705.10528">ArXiv Link</a>, <a href="https://github.com/jachiam/cpo">Code</a>, <a href="http://bair.berkeley.edu/blog/2017/07/06/cpo/">Blog</a>)<br><br>
</li>

<li> <b>Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning</b>, <br>
with Shankar Sastry. <br>
NIPS 2016 Deep RL Workshop. (<a href="https://arxiv.org/abs/1703.01732">ArXiv Link</a>, <a href="https://github.com/jachiam/surprise">Code</a>)<br><br></li>
</ul>

</td>

</tr>
<tr>
<td>
<b>Teaching</b>
</td>
<td>

I've been a GSI (graduate student instructor) for the following courses:

<ul>
<li>CS294-112, Deep Reinforcement Learning, Fall 2017 <a href="http://rll.berkeley.edu/deeprlcourse/">(Course website)</a></li>
<li>EE127/227A, Optimization Models, Spring 2017</li>
</ul>

Some (possibly useful) materials I've made while teaching:
<ul>
<li><a href="http://rll.berkeley.edu/deeprlcourse/f17docs/tf_review_session.pdf">Tensorflow Introduction / Review</a></li>
<li><a href="http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_13_advanced_pg.pdf">Lecture on advanced policy gradient methods (PPO/TRPO)</a></li>
<li><a href="https://github.com/jachiam/optimization-notes">Review notes for convex optimization</a></li>
</ul>

</td>
</tr>

</table>

</body>
</html>
